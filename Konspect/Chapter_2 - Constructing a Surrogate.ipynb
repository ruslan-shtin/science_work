{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Конструирование суррогатной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основная задача этой главы - изучить отображение $y = f(x)$ , производящееся в так называемом \"чёрном ящике\", который скрывает физику этого отображения, преобразующего вектор __x__ в скалярную величину $y$ . Этот черный ящик может принимать форму физического или компьютерного эксперимента, например, кода, реализующего метод конечных элементов, который вычисляет максимальное напряжение (f) для заданных размеров изделия (__x__). Общий метод решения данной проблемы состоит следующем: сначала собираются выходные значения $y^{(1)},y^{(2)},...,y^{(n)}$, каждое из которых является результатом действия отображения $f$ на входные данные $x^{(1)},x^{(2)},...,x^{(n)}$, а затем (на основе наблюдений) ищется функция $\\hat{f}(x)$, которая описывает отображение чёрного ящика $f$ наилучшим образом.  \n",
    "В этой главе мы обсудим основы и некоторые технические детали ряда конкретных типов суррогатных моделей, способных решить поставленную задачу. Однако начнём мы с обсуждения ключевых этапов процесса построения суррогатной модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Процесс Моделирования"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Первая стадия: Подготовка данных и выбор Подхода к моделированию"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В разделе 1 рассматриваются два предварительных шага этого этапа. Первый - это идентификация: с помощью небольшого числа наблюдений определить те параметры входных данных, которые оказывают существенное влияние на $f$; то есть происходит определение кратчайшего вектора проектной переменной $x = \\{x_1, x_2,..., x_k\\}^T$, который, в рассматриваемых диапозонах всех своих компонент может сильно влиять на поведение чёрного ящика. Если бы чёрный ящик был электронным устройством, оснащенным большим набором элементов управления, то этот шаг сводился бы к выявлению $k$ элементов управления, которые при изменении влияют на его поведение. Эта операция осложнена возможными взаимодействиями между элементами управления. На этом этапе также необходимо установить диапазоны различных проектных переменных.  \n",
    "\n",
    "Второй шаг - сбор $n$ штук векторов длины $k$ в список $X = \\{ x^{(1)}, x^{(2)},...,x^{(n)} \\}^T$, таким образом, чтобы этот набор максимально полно представлял собой пространство проектирования. Основная проблема в том, что n часто невелико, т.к. оно ограничено вычислительной и временной стоимостью, приходящейся на каждое наблюдение $f(x_i)$.  \n",
    "Возможно, здесь стоит повторить, что на этом шаге неплохо масштабировать x в единичный куб $[0,1]^k$, что может упростить некоторые последующие математические вычисления и избавить нас от множества проблем многомерного масштабирования.  \n",
    "\n",
    "Поскольку в главе 1 изложен ряд методов для выполнения вышеуказанного, здесь мы рассмотрим следующую фазу процесса - фактическую попытку изучения $f$ с помощью пар данных $\\{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}), ...(x^{(n)},y^{(n)})\\}$. Этот так называемый _процесс обучения под наблюдением или на основе примеров_, по сути, представляет собой поиск в пространстве всех возможных функций $\\hat{f}$, которые воспроизводили бы наблюдения $f$.  \n",
    "\n",
    "Это пространство бесконечно. В конце концов, можно нарисовать любое количество (гипер)поверхностей, прохождящих в определенном диапазоне (с учетом экспериментальной ошибки) известных наблюдений. Однако подавляющее большинство из них будет очень плохо обобщаться; то есть они будут практически бесполезны для прогнозирования ответов на новых данных, в чем и заключается цель данной задачи."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим несколько экстремальный пример функции \"иголка(иголки) в стоге сена\":\n",
    "\n",
    "\n",
    "$$ \\hat{f} = \\begin{cases}\n",
    "   y^{(1)} &\\text{if $x = x^{(1)}$}\\\\\n",
    "   y^{(2)} &\\text{if $x = x^{(2)}$}\\\\\n",
    "   ...\\\\\n",
    "   y^{(n)} &\\text{if $x = x^{(n)}$}\\\\\n",
    "   0 &\\text{иначе}\\\\ \n",
    " \\end{cases} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очевидно, что, хотя все обучающие данные могут быть воспроизведены с помощью этого предиктора, но есть, по крайней мере, в отношении большинства инженерных функций, что-то сильно противоречащее интуиции и тревожащее в том, что он предсказывает 0 везде, кроме обучающих точек. Конечно, есть небольшая вероятность того, что функция действительно выглядит как (1), и по какой-то необычайной случайности нам довелось исследовать её именно там, где находятся все иглы, но это маловероятно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно было бы придумать множество других, возможно, менее надуманных примеров, которые также кажутся какими-то неестественными и, более того, плохо обобщаются. В конечном счете все это говорит о том, что нам нужны какие-то систематические инструменты фильтрации таких бессмысленных предсказателей. Некоторые ученые занимают здесь Байесовскую позицию, например, философия, отстаиваемая Расмуссеном и Уильямсом (2006), заключается в том, чтобы \"заранее дать вероятность каждой возможной функции, причём более высокие вероятности дать функциям, которые мы считаем более вероятными, например, потому что они более гладкие, чем другие функции\".  \n",
    "\n",
    "Подход, который мы будем использовать в дальнейшем, заключается в том, чтобы заранее опредлить структуру $\\hat{f}$ (например, полином третей степени), встроить её в алгоритм моделирования и выполнить поиск по пространству её параметров для настройки приближения к наблюдениям. Например, рассмотрим одну из простейших возможных моделей: $\\hat{f}(x,w) = w^T * x + v$. Изучение f с помощью этой модели подразумевает, что мы определились с ее структурой – это будет гиперплоскость – и процесс подгонки модели состоит в нахождении $k + 1$ параметров (вектор наклона w и перехват v), для которых значение выражения $w^T * x + v$ наилучшим образом соответствует данным (это будет выполнено на второй стадии).  \n",
    "\n",
    "Все вышесказанное часто дополнительно осложняется наличием шума в наблюдаемых откликах (будем считать, что проектные вектора \\textbf{x} никоим образом не повреждены). Мы обсуждали природу этого шума в начале главы 1. Здесь мы концентрируемся на изучении таких данных, для которых иногда присутсвует вероятность ___перенасыщения___.\n",
    "Перенасыщение происходит, когда модель в некотором смысле слишком гибкая и слишком точно соответствует обучающим данным, то есть соответствует шуму, а также фактическому базовому поведению, которое мы пытаемся смоделировать. На второй стадии процесса суррогатного строительства эта проблема _управления сложностью_ решается с помощью процесса оценки параметров модели фиксированной конструкции, но здесь, на этапе выбора типа модели, также требуется некоторое предвидение в этом направлении.  \n",
    "\n",
    "Обычно этот шаг связан с физическими соображениями; то есть выбор метода моделирования зависит от наших ожиданий того, как может выглядеть лежащий в основе ответ. Например, если у нас есть некоторые наблюдения за напряжением в упруго деформированном твердом теле в ответ на небольшие деформации, имеет смысл моделировать напряжение с помощью простого линейного приближения.  \n",
    "Если такое понимание физики недоступно и мы не сможем учесть, скажем, линейность данных на данном этапе, мы в конечном итоге примем сложную, чрезмерно гибкую модель. Это не будет концом света (в конце концов, этап оценки параметров, как мы надеемся, \"линеаризует\" аппроксимацию путем соответствующего выбора параметров, управляющих ее формой), но мы упустим возможность получить алгебраически простую, надежную модель.  \n",
    "\n",
    "И наоборот, если мы ошибочно предположим, что данные получены, скажем, из базового квадратичного процесса, а на самом деле истинная функция $f$ имеет множество пиков и впадин, на этапе оценки параметров мы не сможем компенсировать неудачно выбранную модель. Квадратичная просто будет слишком жесткой, чтобы соответствовать мультимодальной функции, каковы бы ни были ее параметры."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вторая стадия: Оценка параметров и обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте предположим, что на первой стадии мы определили $k$ проектных переменных, получили набор обучающих данных и выбрали общую структуру модели $\\hat{f}(x, w)$, где точная форма модели определяется набором параметров $w$. Теперь мы сталкиваемся с проблемой оценки параметров: как нам выбрать такие $w$, чтобы модель наилучшим образом соответствовала данным? Существует много критериев оценки, но здесь мы обсудим два."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Метод Максимального Правдоподобия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если даны набор параметров $w$ и сама модель $\\hat{f} (x, w)$, то мы можем вычислить вероятность того, что набор данных $\\{(x^{(1)},y^{(1)} \\pm \\epsilon),(x^{(2)},y^{(2)} \\pm \\epsilon),...,(x^{(n)},y^{(n)} \\pm \\epsilon)\\}$ будет получен из $\\hat{f}$ (где $\\epsilon$ - некоторая небольшая постоянная погрешность вокруг каждой точки). Мы можем предположить, что ошибки независимо случайным образом распределены в соответствии с нормальным распределением со стандартным отклонением $\\sigma$, тогда вероятность получения данного набора данных равна  \n",
    "\n",
    "$$P = \\frac{1}{(2\\pi \\sigma^2)^{n/2}} \\prod\\limits_{i=1}^n \\Bigr\\{ exp \\Bigr[ - \\frac{1}{2} \\Bigr( \\frac{y^{(i)} - \\hat{f}(x,w)} {\\sigma}\\Bigl)^2 \\Bigl] \\epsilon \\Bigl\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом и заключается метод максимального правдоподобия: мы предполагаем, что результаты, полученные нашей выборкой, наиболее вероятны (являются мат.ожиданием). Тогда нам нужно максимизировать значение этого выражения. Для упрощения вычислений - прологорифмируем и найдём минумум выражения с обратным знаком, приэтом избавимся от незначимых констант:  \n",
    "\n",
    "$$\\min_w \\sum\\limits_{i=1}^n \\frac{\\Bigr[y^{(i)} - \\hat{f}(x,w)\\Bigl]^2}{2 \\sigma^2} - n \\ln \\epsilon$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заметим, что если мы предполагаем $\\sigma$ и $\\epsilon$ постоянными, то выражение (3) упрощается и принимает хорошо знакомый вид _метода наименьших квадратов_ :  \n",
    "\n",
    "$$\\min_w \\sum\\limits_{i=1}^n \\Bigr[y^{(i)} - \\hat{f}(x,w)\\Bigl]^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Кросс-валидация (скользящий контроль или метод перекрёстной проверки)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перекрёстная проверка состоит из нескольких этапов: \n",
    "1. разбиение данных (случайным образом) на $q$ примерно равных по объёму подмножеств; \n",
    "2. удаление одного из этих подмножеств; \n",
    "3. обучнеие модели на оставшихся $q − 1$ подмножествах. \n",
    "4. Вычисление функции потерь $L$, которая измеряет ошибку между полученным на этапе 3) предиктором и точками в подмножестве, которое мы удалили на этапе 2).\n",
    "\n",
    "Затем возвращаемся к этапу 2) (вернув удалённое подмножество) и удаляем другое ещё не тронутое подмножество. Снова вычисляем функцию потерь $L$ и суммируем за все $q$ итераций.  \n",
    "\n",
    "Более формально, если отображение $\\zeta$ : $\\{1,....n\\} \\rightarrow \\{1,...,q\\}$ описывает распределение $n$ обучающих точек в одно из $q$ подмножеств, а $\\hat{f}^{-\\zeta(i)}(x)$ - значение предиктора (в точке $x$), полученное путем удаления подмножества $\\zeta(i)$ (т.е. подмножества, которому соответствует наблюдение $i$), то мера перекрёстной проверки, которую мы используем здесь в качестве оценки ошибки прогнозирования, равна  \n",
    "\n",
    "$$\\varepsilon_{cv}(w) = \\frac{1}{n} \\sum\\limits_{i=1}^n L \\Bigr[y^{(i)}, \\hat{f}^{-\\zeta(i)}(x^{(i)},w)  \\Bigl]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функцию потерь $L$ можно ввести, как квадрат ошибки, тогда можно переписать уравнение (5) так:  \n",
    "\n",
    "$$\\varepsilon_{cv}(w) = \\frac{1}{n} \\sum\\limits_{i=1}^n L \\Bigr[y^{(i)} - \\hat{f}^{-\\zeta(i)}(x^{(i)},w)  \\Bigl]^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В какой степени уравнение (6) является объективной оценкой истинной ошибки прогнозирования, зависит от выбора $q$. Можно показать, что если $q = n$, то $\\varepsilon_{cv}$ является почти точной оценкой истинной ошибки. Однако дисперсия этой _меры исключения_ может быть очень высокой из-за того, что $n$ подмножеств очень похожи друг на друга. Хасти и др. (2001) рекомендуют компромиссные значения $q = 5$ или $q = 10$. С практической точки зрения, использование меньшего количества подмножеств дает дополнительный бонус в виде снижения вычислительных затрат на процесс перекрестной проверки за счет сокращения количества моделей, которые необходимо подставлять."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Третья стадия: Тестирование модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "218px",
    "width": "361px"
   },
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "769px",
    "left": "110px",
    "top": "180px",
    "width": "165px"
   },
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
