{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Суррогатная модель - это аппроксимация, которая дешевле или удобнее для вычисления, чем основополагающа модель. Наиболее распространенным использованием суррогатных моделей является замена известной дорогостоящей вычислительной модели, когда требуется большое количество повторных вычислений. Другое распространенное применение - это когда мы хотим получить непрерывную функцию из фиксированного набора данных, например, полученых экспериментально или из устаревшего кода. Третье приложение - сглаживание основополагающей модели, возможно, для достижения дифференцируемости при оптимизации на основе градиента.  \n",
    "\n",
    "При обсуждении суррогатных моделей полезно разделять построение и оценку модели, поскольку большинство таких моделей имеют параметры, которые предварительно вычисляются на этапе построения. Здесь мы называем оценку модели ___прогнозом___. Учитывая входные данные $n_x$ и параметры $n_w$, прогнозирование представляет собой следующую оценку:  \n",
    "$$y = f(\\mathbf{x}, \\mathbf{w}),$$  \n",
    "где $\\mathbf{x} \\in \\mathbb{R}^{n_x}$ - это входной вектор данных, $y \\in \\mathbb{R}$ - искомая переменная, а $\\mathbf{w} \\in \\mathbb{R}^{n_w}$ - это вектор параметров модели. Мы называем построение модели ___обучением___ и, следовательно, набор данных - ___точками обучения___. Цель обучения состоит в том, чтобы вычислить параметры модели $\\mathbf{w}$, которые удовлетворяют равенству $(1)$ или приближают его как можно лучше:  \n",
    "$$\\bar{y_i} \\approx f(\\bar{\\mathbf{x}}_i, \\mathbf{w}), \\, \\forall 1\\le i \\le n_t$$  \n",
    "где $(\\bar{\\mathbf{x}}_1,\\bar{y_1}), ... , (\\bar{\\mathbf{x}}_{n_t},\\bar{y_{n_t}})$ - это $n_t$ точек обучения.  \n",
    "\n",
    "Из-за своей широкой применимости и полезности суррогатное моделирование было темой активных исследований в течение десятилетий. В инженерном деле кригинг является одним из наиболее часто используемых методов по нескольким причинам. Во-первых, это наиболее точный метод в целом для небольшого или среднего количества обучающих точек ($n_t < 10^3$). Во-вторых, время прогнозирования и обучения кригингу и хорошо пропорционально размерности признакового пространства $n_x$, что позволяет использовать его в задачах с большой размерностью, где $n_x$ может достигать $O(10^2)$. В-третьих, его стохастическая интерпретация помогает оценивать ошибки прогнозирования через дисперсию точек прогнозирования. Однако к недостаткам кригинга относятся увеличение времени прогнозирования с увеличением количества обучающих точек и склонность к провалу обучения, когда тренировочные точки находятся слишком близко друг к другу. Эти недостатки ограничивают максимальное количество обучающих точек, с которыми может справиться кригинг, что, в свою очередь, ограничивает точность, которая может быть достигнута при наличии большого количества тренировочных точек.  \n",
    "\n",
    "В этой статье мы в первую очередь руководствуемся приложениями, в которых суррогатная модель является частью более крупной модели. Например, суррогатная модель может адаптировать аэродинамические характеристики воздушного судна к условиям полета; здесь суррогат является частью многодисциплинарной модели, которая включает другие дисциплины, представленные другими моделями, которые могут быть или не быть суррогатами. Если набор точек обучения фиксирован, то суррогатную модель можно обучить один раз заранее и использовать повторно при каждом запуске мультидисциплинарной модели. Это имеет место во многих задачах многодисциплинарной оптимизации проектирования (MDO), и это побуждает уделять больше внимания времени прогнозирования, чем времени обучения. Другие приложения, такие как оптимизация на основе суррогатов, придают большее значение меньшему времени обучения, поскольку обучение происходит на каждой итерации оптимизации.  \n",
    "\n",
    "Мы разрабатываем новый метод суррогатного моделирования для задач малой размерности ($n_x \\le 4$), который мы называем ___регуляризованными сплайнами тензорного произведения минимальной энергии (regularized minimal-energy tensor-product splines, RMTS)___. RMTS, как правило, обучается медленнее, чем кригинг, но у него быстрое время прогнозирования, которое не увеличивается с увеличением количества обучающих точек. Более того, он может работать с гораздо большим количеством точек, что означает, что при наличии больших наборов данных, например, когда источником данных является быстрая, но недифференцируемая модель, точность, которая может быть достигнута с помощью RMTS, как ожидается, будет выше, чем с помощью кригинга. Интерес к сплайнам тензорного произведения снизился в последние несколько десятилетий из-за их плохого масштабирования по $n_x$; однако современное вычислительное оборудование смягчает эти ограничения масштабирования и позволяет RTMS масштабировать до четырехмерных задач. Более того, сплайны тензорного произведения позволяют прогнозировать на порядки быстрее, чем кригинг, когда количество обучающих точек велико ($n_t > 10^4$). RMTS использует минимизацию энергии и регуляризацию для повышения точности при работе с небольшими наборами данных и для обработки неструктурированных наборов данных, т.е. точек обучения, не расположенных в структурированной сетке.  \n",
    "\n",
    "RMTS доступен по лицензии с открытым исходным кодом как часть _инструментария суррогатного моделирования (surrogate modeling toolbox, SMT)_. Все проблемы сравнительного анализа, а также другие подходы к суррогатному моделированию, рассмотренные в этой статье, включены в репозиторий SMT, поэтому наши результаты полностью воспроизводимы.  \n",
    "\n",
    "Статья организована следующим образом. В разделе 2 мы рассмотрим некоторые методы суррогатного моделирования, которые обычно используются в инженерии: полиномы, сплайны, искусственные нейронные сети, регрессия опорных векторов, взвешивание по обратному расстоянию, радиальные базисные функции и кригинг. В разделе 3 мы представим уравнения и алгоритмы RMTS. В разделе 4 мы будем использовать набор контрольных показателей для оценки RMTS и сравнения методов суррогатного моделирования с точки зрения времени обучения, времени прогнозирования и точности. Мы также обсудим использование RMTS в практическом контексте MDO, связанном с оптимизацией полетов воздушных судов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обзор методов суррогатного моделирования"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В инженерном деле суррогатная модель также известна как _поверхность отклика_ или как _метамодель_, отражающая идею о том, что она является моделью основополагающей модели. В этой статье мы используем термин _суррогатная модель_ повсюду, чтобы оставаться последовательными, отмечая при этом, что в других контекстах используются разные термины.  \n",
    "Подходы к суррогатному моделированию можно разделить на _интерполяцию_ (если суррогатная модель соответствует истинному значению функции в каждой точке обучающего набора данных) и _регрессию_ (если это не так). Методы регрессии плавно приближают зашумленные данные и включают в себя полиномы, сплайны, искусственные нейронные сети (artificial neural networks, ANN) и регрессию опорных векторов (support vector regression, SVR). Методы интерполяции пытаются плавно и точно подогнать чистые (незашумлённые) данные, и они включают в себя взвешивание обратного расстояния (inverse distance weighting, IDW), радиальные базисные функции (radial basis functions, RBF) и кригинг. Эти методы широко обсуждаются в литературе Ван и Шан (2007); Симпсон(2008); Форрестер(2008) и др.  \n",
    "\n",
    "Поскольку RMTS классифицируется как метод интерполяции, мы кратко рассмотрим методы регрессии и более подробно объясним методы интерполяции. В разделе 4 представлены результаты сравнения RMTS с IDW, RBF и кригингом, поэтому мы также представим уравнения для каждого из них в том виде, в котором они реализованы для сравнительного анализа."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Регрессионные методы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Полиномиальная регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полиномиальная регрессия использует глобальные полиномы низкого порядка от нескольких переменных для аппроксимации обучающих данных. Полиномиальные поверхности отклика были первоначально введены Боксом и Уилсоном (1951). Они обладают преимуществом простоты, что делает их быстрыми и удобными в работе. Однако им не хватает гибкости, и поэтому для многих типов задач они менее точны, чем другие методы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сплайны"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наиболее успешным методом суррогатного моделирования с использованием сплайнов являются многомерные адаптивные регрессионные сплайны (multivariate adaptive regression splines, MARS), разработанные Фридманом (1991). MARS использует базовые функции, которые кусочно линейны в каждом измерении, и адаптивно разрезает и склеивает базовые функции с помощью жадного алгоритма. MARS хорошо масштабируется с размерностью задачи ($n_x$), но недостатком является то, что время обучения и прогнозирования увеличивается с увеличением количества узлов(что в свою очередь связано с точностью)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Искуственные нейронные сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нейросети работают со взаимосвязанным набором узлов, которые вычисляют функцию активации на основе входных данных, точно так же, как нейроны в мозге срабатывают на основе импульсов. Эти узлы расположены слоями ([персептрон](https://ru.wikipedia.org/wiki/%D0%9F%D0%B5%D1%80%D1%86%D0%B5%D0%BF%D1%82%D1%80%D0%BE%D0%BD)), где один слой состоит из $n_x$ входов, другой слой состоит из $n_y$ выходов, а остальные слои известны как _скрытые слои_. По сравнению с типичными методами суррогатного моделирования в инженерии, нейронные сети демонстрируют более медленную сходимость. С другой стороны, они способны решать значительно более сложные задачи, такие как распознавание речи и символов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Метод опорных векторов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVR также имеет свои корни в машинном обучении, но он был успешным в качестве метода для суррогатного моделирования в инженерных приложениях. Обычно он выводится как задача оптимизации, которая находит наиболее “плоское” линейное приближение с заданной точностью. Первоначальное решение использовало скалярное произведение обучающих векторов в целевой функции, а замена этого скалярного произведения другой функцией приводит к общему методу SVR. Выбор функции Гаусса оказывается похожим на RBFs с ядром Гаусса, за исключением того, что он выполняет регрессию с заданным допуском, а не интерполяцию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Методы интерполяции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Метод обратных взвешенных расстояний](https://desktop.arcgis.com/ru/arcmap/10.4/extensions/geostatistical-analyst/how-inverse-distance-weighted-interpolation-works.htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IDW, также известный как метод Шепарда, использует линейную комбинацию результатов на обучающей выборке, где коэффициенты вычисляются исходя из обратного расстояния от точки прогнозирования до каждой точки выборки. Этот метод точно интерполирует неструктурированные данные в $n$-мернфх пространствах, будучи непрерывным и дифференцируемым повсюду.  \n",
    "\n",
    "Пример оригинального IDW интерполятора:  \n",
    "$$f(\\mathbf{x},\\mathbf{w}) = \\begin{cases}\n",
    "    \\bar{y}_i, & \\text{$\\mathbf{x} = \\mathbf{\\bar{x}}$ for some $i$}\\\\\n",
    "    \\frac{\\sum_{i=1}^{n_t} d(\\mathbf{x},\\mathbf{\\bar{x}}_i)^{-p}\\bar{y}_i}{\\sum_{i=1}^{n_t} d(\\mathbf{x},\\mathbf{\\bar{x}}_i)^{-p}}, & \\text{otherwise}\\\\\n",
    "    \\end{cases}$$  \n",
    "где $p>1$ опционально (его можно менять). Хотя функция расстояния не определена в случае, когда предсказание осуществляется в одной из точек обучения, общая функция IDW (определённая так) по-прежнему непрерывна, как и производные до тех пор, пока $p > 1$. Метод IDW уникален тем, что не требует никакой подготовки. Поэтому его преимущество заключается в том, что он может обрабатывать очень большое количество обучающих точек (мы приводим результаты до $n_t = 10^5$). Еще одно преимущество заключается в том, что интерполятор остается в пределах минимума и максимума обучающих точек на всём исследуемом пространстве (Гордон и Уиксом, 1978). Однако IDW медленно прогнозирует, потому что он вычисляет расстояния от точки предсказания до всех точек обучения. Более серьёзным недостатком является то, что производные интерполятора исчезают в каждой точке обучения, поэтому IDW дает плохо дифференциируемый интерполятор даже в простом одномерном случае, когда точки обучения находятся на одной линии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Метод радиальных базисных функций"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RBF определяются как линейная комбинация базисных функций, каждая из которых зависит только от расстояний от точки прогнозирования до каждой обучающей точки. Коэффициенты в линейной комбинации определяются путём решения линейной системы линейных уравнений, матрица которой обычно является плотной. Радиально изменяющиеся базисные функции обычно дополняются полиномиальными функциями для отражения общих тенденций.  \n",
    "\n",
    "При представлении уравнений RBF мы используем отображение $p_i : \\mathbb{R}^{n_x} \\rightarrow \\mathbb{R}$ для обозначения $i$-ой из $n_p$ полиномиальных функций и $\\phi_i : \\mathbb{R}^{n_x} \\rightarrow \\mathbb{R}$ для обозначения $i$-ой из $n_p$ базисных функций. Прогнозирование с использованием RBF делается следующим образом:  \n",
    "$$f(\\mathbf{x},\\mathbf{w}) = \\sum\\limits_{i=1}^{n_p}p_i(\\mathbf{x})a_i + \\sum\\limits_{i=1}^{n_t}\\phi_i(\\mathbf{x})b_i$$  \n",
    "где $a_i \\in \\mathbb{R}$ - вес $i$-ой полиномиальной функции и $b_i \\in \\mathbb{R}$ - вес $i$-ой базисной функции. Таким образом, $\\mathbf{w}$ состоит из коэффициентов $a_1,...,a_{n_p}$ и $b_1,...,b_{n_t}$.  \n",
    "В матричном виде предыдущее уравнение примет вид:  \n",
    "$$\\mathbf{y = P a + \\Phi b}$$  \n",
    "где $\\mathbf{y} \\in \\mathbb{R}^n, \\mathbf{P} \\in \\mathbb{R}^{n \\times n_p}, \\mathbf{a} \\in \\mathbb{R}^{n_p}, \\mathbf{\\Phi} \\in \\mathbb{R}^{n \\times n_t}, \\mathbf{b} \\in \\mathbb{R}^{n_t}$.  \n",
    "Тогда обучение методом RBF состоит в решении системы линейных уравнений:  \n",
    "$$\\begin{bmatrix}\n",
    "\\bar{\\mathbf{\\Phi}} & \\bar{\\mathbf{P}}\\\\\n",
    "\\bar{\\mathbf{P}}^T & 0\n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix}\n",
    "\\mathbf{b} \\\\\n",
    "\\mathbf{a}\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "\\bar{\\mathbf{y}} \\\\\n",
    "0\n",
    "\\end{bmatrix}$$  \n",
    "где $\\displaystyle \\mathbf{\\Phi} \\in \\mathbb{R}^{n \\times n_t}$ -значения базисных функций при $\\bar{\\mathbf{x}}_1, ..., \\bar{\\mathbf{x}}_{n_t}$, $\\bar{\\mathbf{P}} \\in \\mathbb{R}^{n_t \\times n_p}$ - это значения полиномиальных функций при $\\bar{\\mathbf{x}}_1, ..., \\bar{\\mathbf{x}}_{n_t}$, и $\\bar{\\mathbf{y}} \\in \\mathbb{R}^{n_t}$ - это значения искомой функции в обучающих точках, записанные в матричном виде. Линейная система (4) получена путем выполнения начального полиномиального приближения методом наименьших квадратов для определения общих тенденций перед применением базисных функций к результирующей ошибке. Система наименьших квадратов равна $\\mathbf{\\bar{P}a} = \\mathbf{\\bar{y}}$, а соответствующие нормальные уравнения таковы: $\\mathbf{\\bar{P}^T \\bar{\\Phi}^{−1}\\bar{P}a} = \\mathbf{\\bar{P}^T \\bar{\\Phi}^{−1}\\bar{y}}$. Второй шаг решает линейную систему $\\mathbf{\\bar{\\Phi}b} = \\bar{\\mathbf{y}}−\\bar{\\mathbf{P}}\\mathbf{a}$.  \n",
    "\n",
    "Существует множество вариантов базовых функций, но наиболее распространенными являются следующие:  \n",
    "* функции Гаусса: $\\phi_i(\\mathbf x) = \\exp\\Bigl(-\\frac{\\| \\mathbf x - \\bar{\\mathbf x}_i \\|}{r_0^2}\\Bigr)$  \n",
    "* тонкий сплайн пластины: $\\phi_i(\\mathbf x) = \\|\\mathbf x - \\bar{\\mathbf x}_i \\|^2 \\ln{\\|\\mathbf x - \\bar{\\mathbf x}_i \\|}$\n",
    "* мультиквадратичные фукнции: $\\phi_i(\\mathbf x) = \\sqrt{\\|\\mathbf x - \\bar{\\mathbf x}_i \\|^2 + r_0^2}$,  \n",
    "где $r_0$ - изменяемый параметр. Тонкие сплайны пластины имеют одно преимущество: у них нет параметров, которые необходимо настраивать. Для мультиквадратичных функций были проведены исследования по оптимальному выбору параметра $r_0$ для повышения точности интерполяции (Карлсон и Фоули, 1991; Канса и Карлсон, 1992).  \n",
    "\n",
    "Радиальные базисные функции очень точны для многих задач, и им требуется немного времени на обучение для небольших наборов данных. Однако одним из недостатков RBF является то, что они очень восприимчивы к паразитным колебаниям с данными, которые быстро меняются или неравномерно распределены во входных данных. По мере увеличения количества точек обучения время прогнозирования также увеличивается, и матрица линейных уравнений (4) может стать плохо обусловленной и привести к плохому обучению. Еще одним недостатком является наличие проблемно-зависимых параметров настройки. Как мы увидим в следующем разделе, кригинг использует те же базовые уравнения, что и RBF, но эквиваленты базовых функций кригинга настраиваются автоматически.  \n",
    "Для повышения точности были широко изучены _анизотропные базисные функции_, значения которых зависят не только от расстояния до центра(Митасова и Митас, 1993; Динь и др., 2001; Кашиола и др., 2007, 2010; Битсон и др., 2010; Ю и Турк, 2013). Другой модификацией RBF является использование _компактно поддерживаемых базисных функций_ для достижения разреженности для повышения эффективности (Wendland, 1995; Wu, 1995; Buhmann, 2000). Компактные базисные функции несколько повышают эффективность оценки, а оптимально подобранные базисные функции повышают точность. Однако основным недостатком RBF остается то, что они не так надежны, как интерполяторы, основанные на минимизации энергии, особенно с обучающими точка, которые значительно различаются по расстоянию или целевому значению."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Кригинг](https://ru.wikipedia.org/wiki/%D0%9A%D1%80%D0%B8%D0%B3%D0%B8%D0%BD%D0%B3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кригин, также известный как регрессия на основе гауссовских процессов (Gaussian process regression, GPR), можно рассматривать как обобщение подхода RBF, как мы покажем далее. Однако кригинг интерпретирует интерполируемую функцию как случайный процесс, где значение функции в каждой точке области рассматривается как отдельная случайная величина, которая коррелирует со всеми остальными. Первоначально кригинг был применён в геостатистике, но после он стал широко использоваться в численном моделировании и проектировании.  \n",
    "Кригинг включает в себя общие трендовые модели (модели для определения доминирующего поведения функции) аналогично глобальным полиномам, которые используются для RBF. _Простой кригинг_ использует известное среднее значение; _обычный кригинг_ использует постоянное, но неизвестное среднее значение; а _универсальный кригинг_ предполагает общее полиномиальное среднее. Универсальный кригинг используется для повышения точности за счет небольшого увеличения вычислительной сложности, но численные эксперименты показывают, что это не всегда так (Циммерман и др., 1999).  \n",
    "Будем использовать обозначения как для RBF для функций полиномиального тренда, $p_i : \\mathbb{R}^{n_x} \\rightarrow \\mathbb{R}$, и обозначим корреляцию между точкой прогнозирования и $i$-ой точкой обучения как $\\psi_i : \\mathbb{R}^{n_x} \\rightarrow \\mathbb{R}$. Тогда прогнозирование в случае универсального кригинга примет вид:  \n",
    "$$f(\\mathbf{x},\\mathbf{w}) = \\sum\\limits_{i=1}^{n_p}p_i(\\mathbf{x})a_i + \\sum\\limits_{i=1}^{n_t}\\psi_i(\\mathbf{x})b_i$$  \n",
    "где $a_i \\in \\mathbb{R}$ - вес $i$-ой полиномиальной компоненты функции тренда и $b_i \\in \\mathbb{R}$ - вес $i$-ой обучающей точки. Поэтому вектор $\\mathbf w$ состоит из весов $a_1, ...,a_{n_p}$ и  $b_1, ..., b_{n_t}$.  \n",
    "В матричном виде предсказание примет вид:  \n",
    "$$\\mathbf{y} = \\mathbf{P}\\mathbf{a} + \\mathbf{\\Psi}\\mathbf{b}$$,  \n",
    "где $\\mathbf{y} \\in \\mathbb{R}^n$, $\\mathbf{P} \\in \\mathbb{R}^{n \\times n_p}$, $\\mathbf{a} \\in \\mathbb{R}^{n_p}$, $\\mathbf{\\Psi} \\in \\mathbb{R}^{n \\times n_t}$, и $\\mathbf{b} \\in \\mathbb{R}^{n_t}$.  \n",
    "Веса могут быть вычислены путём решения линейной систему уравнений:  \n",
    "$$\\begin{bmatrix}\n",
    "\\bar{\\mathbf{\\Psi}} & \\bar{\\mathbf{P}}\\\\\n",
    "\\bar{\\mathbf{P}}^T & 0\n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix}\n",
    "\\mathbf{b} \\\\\n",
    "\\mathbf{a}\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "\\bar{\\mathbf{y}} \\\\ \n",
    "0\n",
    "\\end{bmatrix}$$  \n",
    "\n",
    "которая идентична системе для RBF, только вместо радиальных базисных функций $\\phi$ используются функции корреляции $\\psi$.  \n",
    "\n",
    "Аналогично RBFs мы можем получить уравнение (10), применив двухэтапный процесс. Первый шаг и связанное с ним вычисление $\\mathbf{a}$ идентично случаю с RBF — коэффициенты полиномиальных функций могут быть найдены с помощью взвешенного решения по методу наименьших квадратов. Второй шаг и вычисление $\\mathbf b$ следуют из уравнения $\\mathbf y − \\mathbf{P}\\mathbf{a} = \\mathbf{\\Psi}\\bar{\\mathbf{\\Psi}}^{-1}(\\bar{\\mathbf{y}} − \\bar{\\mathbf{P}}\\mathbf{a})$ (см. дополнение, раздел A), что легко можно показать для решения уравнения (10).  \n",
    "\n",
    "Кригинг отличается от RBF в выборе функции ковариации $\\psi$. Кригинг использует обобщённую форму для $\\psi$ и вычисляет параметры с помощью оптимизации методом максимального правдоподобия. Наиболее часто используется форма, выраженная Тоалом и др. (2008):  \n",
    "$$\\psi_i(\\mathbf x) = \\sigma^2 \\exp{\\Bigl( - \\sum\\limits_{k=1}^{n_x} \\theta_k \\|x_k - \\bar{x}_{i,k}\\|^{p_k}\\Bigr)}$$,  \n",
    "где $\\sigma^2$ - это дисперсия, и $\\theta_1,...,\\theta_n$ и $p_1, ..., p_{n_x}$ - _гиперпарматры_.  \n",
    "\n",
    "Оптимальные гиперпараметры для данного набора обучающих данных могут быть найдены путем вычисления _оценки максимального правдоподобия_ (maximum likelihood estimator, MLE). В этом контексте функция правдоподобия принимает в качестве входных данных значения гиперпараметров и возвращает вероятность того, что случайные величины принимают фактические значения выходных переменных в точках обучения. Поэтому мы настраиваем значения гиперпараметров, чтобы максимизировать эту вероятность. Другими словами, параметры подбираются таким образом, чтобы полученная обучающая выборка была наиболее вероятна.  \n",
    "\n",
    "Аспекты автоматизации и оптимизации подхода MLE привлекательны, но корреляционная матрица может быть плохообсловлена, а логарифм функции правдоподобия, которая используется в качестве целевой функции, может быть мультимодальным (Мартин и Симпсон, 2005). Существуют два альтернативных подхода к MLE: подгонка ковариационной функции к эмпирически определенной вариограмме и кросс-валидация методом \"leave-one-out\", когда на валидацю оставляется только одна точка (Митчелл и Моррис, 1992). Кросс-валидация позволяет избежать численных проблем, связанных с максимизацией функции логарифмического правдоподобия, но она может быть непоследовательной и иногда работает намного хуже (Мартин и Симпсон, 2005).  \n",
    "\n",
    "Таким образом, кригинг имеет много преимуществ, но для некоторых типов задач существуют численные проблемы, которые ограничивают его надежность. С точки зрения преимуществ, кригинг хорошо масштабируется с размерностью ($n_x$), и он обладает привлекательным свойством, заключающимся в том, что обучающие кластеризованные точки в целом имеют меньший вес, чем те, которые расположены в разреженной области проектного пространства. Что касается недостатков, размер линейных систем, которые решает кригинг, привязан к количеству точек обучения, и для больших задач это может быть узким местом. Это побудило к исследованиям кригинга с использованием высокопроизводительных вычислений и параллелизма (Керри и Хоуик, 1998), приближений с фиксированным рангом с использованием формулы Шермана-Моррисона-Вудбери (Кресси и Йоханнессон, 2008) и аппроксимации обратной ковариационной матрицы Лейтхеда и Чжана (2007) с использованием формулы обновления BFGS. Другим недостатком является то, что вычисление гиперпараметров кригинга может оказаться сложной задачей. Определение эмпирической вариограммы - это ручной процесс, который не всегда надежен, а MLE может не сходиться, особенно при больших задачах; мы наблюдали это в численных экспериментах. Альтернативой является ручной выбор и настройка функции ковариации, и в этом случае метод ничем не отличается от RBF интерполяции."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Метод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сейчас мы разрабатываем суррогатную модель RMTS. Она аналогична IDW, RBF и кригингу в представлении результатов прогнозирования в виде линейной комбинации результатов обучения. Как и для RBF и кригинга, результат прогнозирования для RMTS также может быть интерпретирован как линейная комбинация базисных функций, где коэффициенты предварительно вычисляются во время обучения. Однако, в отличие от RBF и кригинга, RMTS не требует привязки количества базисных функций к количеству точек обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Общий вид"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уравнение прогнозирования для RMTS задаётся следующим образом:\n",
    "$$f(\\mathbf{x},\\mathbf{w}) = \\sum\\limits_{i_1,\\dots,i_{n_x} \\in I} b_{1,i_1}(x_1) \\cdot\\cdot\\cdot b_{n_x,i_{n_x}}(x_{n_x})w_{i_1,...,i_{n_x}}$$  \n",
    "где $\\mathbf x \\in \\mathbb{R}^{n_x}$ , $\\displaystyle b_{k,i}:\\mathbb{R} \\rightarrow \\mathbb{R}$ - это $i$-ая базисная функция над $k$-ого признака, и $w_{i_1,...,i_{n_x}}$ - это коэффициент в этом многомерном [сплайне](https://ru.wikipedia.org/wiki/%D0%A1%D0%BF%D0%BB%D0%B0%D0%B9%D0%BD).\n",
    "\n",
    "В матричных обозначениях уравнение прогнозирования примет вид:  \n",
    "$$\\mathbf{y} = \\mathbf{F}\\mathbf{w}$$\n",
    "где $\\mathbf{F} \\in \\mathbb{R}^{n \\times n_w}$ и $\\mathbf{w} \\in \\mathbb{R}^{n_w}$. Поскольку сплайны являются кусочными многочленами, каждый коэффициент влияет только на часть области; следовательно, каждая базисная функция имеет конечный носитель (множество, где её значение отлично от нуля), а матрица $\\mathbf F$ разрежена.\n",
    "\n",
    "Методология RMTS позволяет использовать любой тип сплайнов. Однако сплайны с равномерным расстоянием между узлами требуют меньшее время для прогнозирования. Если же расстояние между узлами сплайна неравномерно, необходимо выполнить поиск по всем точкам прогнозирования и по каждому из измерений $n_x$, чтобы найти интервал узлов, содержащий входное значение, а это увеличивает время прогнозирования. Для этой статьи и в пакете SMT были реализованы два типа сплайнов: B-сплайны с равномерными векторами узлов и кубические Эрмитовые сплайны.  \n",
    "\n",
    "Мы называем реализацию B-сплайна RMTB, где B обозначает _B-сплайны_. В реализации SMT RMTB степень B-сплайнов и количество контрольных точек могут быть выбраны произвольно, но с ограничением, что количество контрольных точек должно быть не меньше степени  B-сплайна. Обычно с B-сплайнами более высокого порядка достигается лучшая точность, но их время обучения и прогнозирования намного выше, потому что матрица $\\mathbf{F}$ становится менее разреженной. Как правило, используются B-сплайны третьего или четвертого порядка. B-сплайны имеют более высокую гладкость, чем кубические сплайны Эрмита.\n",
    "\n",
    "Реализацию Эрмитовых кубических сплайнов мы называем RMTC, где C обозначает _Эрмитовы кубические сплайны_. В реализации SMT RMTC единственным изменяемым параметром является количество кубических элементов в каждом измерении. Каждый кубический элемент представляет собой $n_x$-мерный гиперкуб. Его $4^{n_x}$ коэффициентов многомерного полинома однозначно определяются $2^{n_x}$ значениями интерполяции и производными в его $2^{n_x}$ узлах. В каждом узле одни и те же значения и производные распределяются между всеми элементами, которые совместно используют узел, поэтому нам гарантируется $C^1$ непрерывность."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "274px",
    "width": "566px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
